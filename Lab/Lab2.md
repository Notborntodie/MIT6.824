[TOC]

# Lab2



## 导言

这是一系列实验室的第一部分，您将在其中构建一个容错的键值存储系统。在此实验室中，您将实现Raft，一种复制状态机协议。在下一个实验室中，您将在Raft之上构建一个键/值服务。然后，您将把您的服务“分片”到多个复制状态机上，以获得更高的性能。

复制服务通过在多个复制服务器上存储其状态（即数据）的完整副本来实现容错。复制使得服务即使某些服务器遭遇故障（崩溃或破碎或不稳定的网络）仍能继续运行。挑战在于故障可能导致副本保存不同的数据副本。

Raft将客户端请求组织成称为**日志**的序列，并确保所有复制服务器看到相同的日志。每个副本按日志顺序执行客户端请求，将其应用于其服务状态的本地副本。由于所有活动副本都看到相同的日志内容，它们都按相同的顺序执行相同的请求，因此继续具有相同的服务状态。如果服务器失败但稍后恢复，Raft会负责使其日志保持最新。只要至少有大多数服务器处于活动状态并且可以互相通信，Raft就会继续运行。如果没有这样的大多数，Raft将不会继续，但一旦大多数可以再次通信，它就会恢复到离开的地方。

在此实验室中，您将实现Raft作为**Go对象类型和相关方法**，旨在用作较大服务中的模块。一组Raft实例使用RPC相互通信以维护复制的日志。您的Raft接口将支持一个无限序列的编号命令，也称为**日志条目**。条目使用索引号编号。具有给定索引的日志条目最终将被提交。此时，您的Raft应将日志条目发送到较大的服务以执行。

您应遵循扩展Raft论文中的设计，特别是注意图2。您将实现论文中的大部分内容，包括保存持久状态并在节点失败后读取它并重新启动。您不会实现集群成员资格更改（第6节）。



![截屏2023-03-25 17.35.39](http://cdn.zhengyanchen.cn/img202303251735036.png)

### 状态设计

在Raft中，服务器的状态被分为持久状态和易失状态。这里是它们的设计：

**所有服务器上的持久状态**（在响应RPC之前更新到稳定存储）：

- `currentTerm`：服务器已知的最新任期（首次启动时初始化为0，单调递增）。
- `votedFor`：在当前任期中获得投票的候选者ID（如果没有，则为null）。
- `log[]`：日志条目数组；每个条目包含状态机的命令以及领导者接收条目时的任期（第一个索引为1）。

**所有服务器上的易失状态**：

- `commitIndex`：已知提交的最高日志条目的索引（初始化为0，单调递增）。
- `lastApplied`：应用到状态机的最高日志条目的索引（初始化为0，单调递增）。

==`commitIndex` 是一个全局的概念，表示在整个集群中已知已提交的最高日志条目。相比之下，`lastApplied` 是一个本地概念，表示在某个特定服务器上已经应用到状态机的最高日志条目==

具体见后面的问题

**领导者上的易失状态**（在选举后重新初始化）：

- `nextIndex[]`：针对每个服务器，要发送给该服务器的下一个日志条目的索引（初始化为领导者最后一个日志条目的索引+1）。
- `matchIndex[]`：针对每个服务器，已知在服务器上复制的最高日志条目的索引（初始化为0，单调递增）。

这些状态变量对于实现Raft协议至关重要。持久状态变量需要在响应RPC之前存储到稳定的存储器（例如磁盘）中，以便在服务器崩溃和重新启动后仍能保留它们的值。易失状态变量则存储在内存中，并且在服务器重新启动后会丢失。领导者上的易失状态在每次选举后都会重新初始化。通过这些状态变量，Raft协议能够在服务器之间同步日志并保证容错性。



翻译 您可能会发现本**指南**(https://thesquareplanet.com/blog/students-guide-to-raft/)以及有关并发的**锁**(https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt)和结构(https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt)的建议很有用。 从更广泛的角度来看，请查看 Paxos、Chubby、Paxos Made Live、Spanner、Zookeeper、Harp、Viewstamped Replication 和 Bolosky 等。 （注意：学生指南是几年前写的，特别是第 2D 部分后来发生了变化。在盲目遵循之前，请确保您理解为什么特定的实施策略是有意义的！）

请记住，本实验中最具挑战性的部分可能不是实施您的解决方案，而是对其进行调试。 为了帮助应对这一挑战，您可能希望花时间思考如何使您的实现更易于调试。 您可以参考指南页面和这篇关于有效打印语句的博客文章。

我们还提供了 Raft 交互图，可以帮助阐明您的 Raft 代码如何与它之上的层交互。



<img src="http://cdn.zhengyanchen.cn/img202303151702417.png" alt="截屏2023-03-15 17.02.21"  />



本实验分为四个部分。 您必须在相应的截止日期提交每个部分。

## 准备

在开始实现Raft之前，请确保已获得实验室的源代码。主要的实现将添加到`raft/raft.go`文件中。

## 接口实现：

在 `raft/raft.go` 文件中添加代码以实现 Raft。在该文件中，您将找到骨架代码以及如何发送和接收 RPC 的示例。您的实现必须支持以下接口，测试器和（最终）您的键/值服务器将使用这些接口。在 raft.go 的注释中，您将找到更多详细信息。

```go
// 创建一个新的 Raft 服务器实例:
rf := Make(peers, me, persister, applyCh)

// 开始对新日志条目达成一致:
rf.Start(command interface{}) (index, term, isleader)

// 询问一个 Raft 服务器它的当前任期，以及它是否认为自己是领导者
rf.GetState() (term, isLeader)

// 每次将新条目提交到日志时，每个 Raft 节点
// 应该向服务（或测试器）发送一个 ApplyMsg。
type ApplyMsg

```

1. 服务通过调用 `Make(peers,me,…)` 创建一个 Raft 节点。`peers` 参数是 Raft 节点（包括此节点）的网络标识符数组，用于进行 RPC。`me` 参数是此节点在 `peers` 数组中的索引。

2. `Start(command)` 会要求 Raft 开始处理将命令附加到复制日志的过程。`Start()` 应立即返回，而不等待日志追加完成。服务期望您的实现对于每个新提交的日志条目都向 `Make()` 的 `applyCh` 通道参数发送一个 `ApplyMsg`。

`raft.go` 包含了发送 RPC（`sendRequestVote()`）和处理传入 RPC（`RequestVote()`）的示例代码。您的 Raft 节点应使用 `labrpc` Go 包（源代码位于 `src/labrpc`）进行 RPC 交互。测试器可以告诉 `labrpc` 延迟 RPC、重新排序它们并丢弃它们，以模拟各种网络故障。虽然您可以暂时修改 `labrpc`，但请确保您的 Raft 能与原始的 `labrpc` 一起工作，因为我们将使用它来测试和评分您的实验室。

==您的 Raft 实例必须仅通过 RPC 进行交互==；例如，它们不允许使用共享的 Go 变量或文件进行通信。

后续的实验室将基于此实验室，因此请确保为编写可靠的代码留出足够的时间。





## 与gpt的对话

### 那么所谓的raft 服务需要部署在每个储存键值的数据服务器？

不完全正确。Raft并不是一个储存键值的数据服务器，而是一个分布式共识算法。在一个分布式系统中，Raft算法可以用于实现某个共识（例如选举领导者、复制状态机等）从而提高系统的可用性和容错性。在键值存储系统中，通常会将多个实例（例如储存节点）组成一个集群，使用Raft算法选举出一个领导者负责响应客户端的请求，然后使用Raft算法来复制领导者的状态到其他节点上，以实现高可用性和容错性。因此，每个储存键值的数据服务器并不需要部署一个Raft服务，而是需要参与到Raft算法中组成一个Raft集群。

### 那么没有master嘛？

在Raft中，没有专门的Master节点，所有的节点都有机会成为领导者。当集群中没有领导者时，节点会通过竞选机制选举一个新的领导者。这种设计能够提高系统的可用性，因为即使某个节点失效，其他节点仍然可以继续提供服务。

### 领导者做哪些工作，和master类似？那么谁来做GFS的主副本的工作？

在 Raft 中，领导者（Leader）负责接收客户端请求并将其转换为日志条目（Log Entries），然后将这些日志条目复制到其他的节点（Followers）上，以确保数据的一致性。同时，领导者还负责处理节点之间的通信、维护心跳机制、处理选举等操作。

与 GFS 不同的是，Raft 的设计只针对数据复制的问题，而没有涉及到数据存储、读取等更高层次的抽象。在实现键值存储时，可以使用 Raft 算法作为底层来实现数据的复制和一致性，但仍需要额外的代码来实现数据的存储和读取。因此，对于 GFS 中主副本的工作，需要另外的机制来实现。

### leader得到日志条目的顺序是否和GFS中主副本给日志排序很像

Raft 中的 Leader 通过 AppendEntries RPC 将新的日志条目追加到所有的 Follower 上，确保 Follower 的日志与 Leader 保持一致。在此过程中，Leader 需要维护每个 Follower 的日志复制进度，如果某个 Follower 落后于 Leader，Leader 需要发送与该 Follower 日志一致的日志条目。这个过程类似于 GFS 中主副本将数据分块后分别存储到各个 ChunkServer 上，主副本需要维护每个 ChunkServer 的数据复制进度，如果某个 ChunkServer 落后于主副本，主副本需要发送与该 ChunkServer 数据一致的数据块。

虽然这两个过程有相似之处，但是它们的实现机制和目的不同。Raft 中的 Leader 通过维护日志一致性来实现状态机复制，而 GFS 中的主副本通过维护数据块一致性来实现数据冗余和容错。

### 是否可以用GFS实现数据的读取和存储，然后使用Raft来保持数据的一致性？

是的，可以使用 GFS 来实现数据的读取和存储，然后使用 Raft 来保持数据的一致性。在这种情况下，GFS 可以充当底层存储系统，用于持久化数据和元数据，而 Raft 可以用于维护数据的一致性和可用性。当客户端请求读取或写入数据时，GFS 可以处理实际的 I/O 操作，而 Raft 则可以确保数据副本之间的一致性。

在这种架构下，Raft 将会使用 GFS 作为其存储层，它将使用 GFS 存储日志、状态和其他 Raft 元数据。Raft 领导者将负责接收写入请求并将它们复制到其他副本上，并使用 GFS 来持久化复制的日志。同时，Raft 跟随者将从领导者处复制日志，并将它们存储在 GFS 中。

在读取数据时，客户端可以直接从 GFS 中读取数据，因为 GFS 可以提供高效的读取。然而，为了确保读取到的是最新的数据，客户端也可以向 Raft 领导者发送读请求，领导者将查询其本地日志并返回最新的数据。在这种情况下，Raft 将充当一个共享状态的协调者，以确保客户端读取到的数据是最新的，并且具有一致的视图。

总之，使用 GFS 作为底层存储系统，并结合 Raft 来保持数据的一致性和可用性，是一种高效且可靠的方法，可以用于构建分布式系统。

### commitIndex和lastApplied 难道不是一个意思嘛，因为日志被应用到状态机才表示被提交不是嘛？

`commitIndex` 和 `lastApplied` 是两个不同的概念，尽管它们看起来可能很相似。让我们详细了解它们之间的区别：

- `commitIndex`：这是已知已提交的最高日志条目的索引。当领导者确定一条日志条目在大多数服务器上都已复制时，该条目将被认为是“已提交”的。领导者会更新自己的 `commitIndex`，并将这个信息通知其他服务器，以便它们也能更新它们的 `commitIndex`。注意，在这一点上，这些已提交的日志条目尚未应用到状态机中。
- `lastApplied`：这是已应用到状态机的最高日志条目的索引。当服务器知道某个日志条目已经被提交时，它将该日志条目应用到本地状态机中，并更新 `lastApplied`。这个过程通常在领导者通知其他服务器更新 `commitIndex` 之后进行。

简而言之，`commitIndex` 是一个全局的概念，表示在整个集群中已知已提交的最高日志条目。相比之下，`lastApplied` 是一个本地概念，表示在某个特定服务器上已经应用到状态机的最高日志条目。虽然在理想情况下，这两者可能非常接近，但它们可能会在不同的服务器上以不同的速度更新，因此需要独立跟踪。

### nextIndex和matchIndex的值不是只差一个1嘛，为啥需要维护两个？另外nextIndex放在易失状态这我理解因为在选举后重新初始化，但是commitIndex放在易失状态是因为速度原因嘛？

`nextIndex` 和 `matchIndex` 是两个不同的概念，虽然它们之间可能存在一定的关联，但它们用于解决不同的问题。让我们详细了解它们的区别：

- `nextIndex[]`：针对每个服务器，要发送给该服务器的下一个日志条目的索引。领导者使用 `nextIndex` 来跟踪将要发送给其他服务器的日志条目。当领导者向其他服务器发送日志条目时，它会根据 `nextIndex` 的值来确定从哪个日志条目开始。如果领导者发现某个服务器落后了，它可以通过减小 `nextIndex` 值来尝试重新发送较旧的日志条目。因此，`nextIndex` 有助于领导者管理与其他服务器之间的日志同步。
- `matchIndex[]`：针对每个服务器，已知在服务器上复制的最高日志条目的索引。领导者使用 `matchIndex` 来跟踪已经在其他服务器上成功复制的日志条目。当领导者收到其他服务器的确认信息时，它会更新与该服务器对应的 `matchIndex` 值。`matchIndex` 对于确定哪些日志条目可以被提交至关重要，因为领导者需要确保一个日志条目在大多数服务器上都已复制，然后才能将其提交。

虽然 `nextIndex` 和 `matchIndex` 在某些情况下可能只相差1，但它们表示的是不同的信息，并且需要分开维护。

至于为什么将 `commitIndex` 放在易失状态中，这是因为 `commitIndex` 可以通过与其他服务器的通信来重新计算。在重新选举领导者后，新领导者会与其他服务器同步日志，并重新计算出正确的 `commitIndex`。将 `commitIndex` 放在易失状态中可以减少磁盘I/O操作，从而提高性能。同时，由于 `commitIndex` 可以在领导者重新选举后重新计算出来，将其放在易失状态中不会对系统的正确性造成影响。

### 这和GFS的部分设计很像，即如果这些数据结构可以通过分布式系统的通讯来恢复那么就放在内存，而如果crash了重启后会影响系统正确性的数据就放在磁盘，这是否可以认为是分布式系统的特点

是的，你的理解是正确的。在许多分布式系统中，包括GFS（Google文件系统）和Raft，设计者经常将数据结构分为两类：易失状态和持久状态。这种分类的主要原因是在分布式系统中，需要考虑服务器可能会发生故障并在一段时间后恢复。为了保证系统的正确性和持久性，设计者需要在这些情况下作出合适的决策。

将数据结构分为易失状态和持久状态可以帮助设计者实现以下目标：

1. **性能**：易失状态通常存储在内存中，访问速度更快。因此，将那些可以通过与其他服务器通信来恢复的数据结构放在内存中，可以提高系统的性能。
2. **正确性**：对于那些在服务器崩溃和重启后可能影响系统正确性的数据结构，将它们存储在磁盘上（即持久状态）是至关重要的。这样做可以确保即使服务器发生故障，这些关键数据也不会丢失。
3. **容错性**：在分布式系统中，服务器可能会因为硬件故障、网络问题等原因发生故障。通过将关键数据结构存储在磁盘上，并在重启后将其重新加载到内存中，可以提高系统的容错性。

所以，是的，将数据结构分为易失状态和持久状态可以被认为是分布式系统设计的一个重要特点。这有助于在保证系统正确性的同时，提高性能和容错性。



### 帮我回顾一下GFS这方面的设计，详细解释

Google文件系统（GFS）是一个可扩展的、分布式的文件系统，设计用于大规模数据存储和处理。GFS的设计目标是实现高性能、高可靠性、高容错性和高可用性。让我们回顾一下GFS在易失状态和持久状态方面的设计：

1. **Master节点**：GFS中有一个中心化的Master节点，负责元数据的管理，例如文件和目录的命名空间、文件到chunk的映射、chunk的副本位置等。为了提高性能，Master将所有的元数据都存储在内存中。这意味着元数据的访问速度非常快，从而提高了整个文件系统的性能。

   但是，将元数据存储在内存中会导致易失性。为了确保系统的正确性和持久性，GFS Master会将关键元数据持久化到磁盘上。这些关键元数据包括文件和目录的命名空间、文件到chunk的映射等。为此，Master将这些关键元数据存储在名为“操作日志”的持久存储中。操作日志会在磁盘上持久保存，并在Master节点启动或重新启动时加载到内存中。

2. **Chunk服务器**：GFS将文件分为多个固定大小的数据块，称为“chunk”。每个chunk由一个全局唯一的ID标识。为了实现容错性，GFS将每个chunk的多个副本存储在不同的Chunk服务器上。这些副本之间是同步的，确保数据的一致性。

   Chunk服务器本身并不维护元数据。它们只关心存储在本地磁盘上的chunk数据。当Master节点需要访问或修改chunk时，它会将请求发送给相应的Chunk服务器。这种设计简化了Chunk服务器的实现，并且可以将磁盘I/O操作局限在Chunk服务器上，从而降低了Master节点的负载。

总之，GFS通过将元数据存储在Master节点的内存中以提高性能，同时将关键元数据持久化到磁盘上以确保正确性。在Chunk服务器端，GFS将数据副本分布在多个服务器上，以实现高容错性。这种易失状态和持久状态的设计在很大程度上帮助GFS实现了其高性能、高可靠性和高容错性的目标。



### 这里的“每次将新条目提交到日志时，每个 Raft 节点应该向服务（或测试器）发送一个 ApplyMsg。” 是说服务器日志添加一个新日志条目的时候还是日志的里面一个条目被应用到状态机的时候?

这里的 "每次将新条目提交到日志时，每个 Raft 节点应该向服务（或测试器）发送一个 ApplyMsg。" 是指当日志中的一个条目被提交（即达成一致）并准备应用到状态机时，每个 Raft 节点应该向服务（或测试器）发送一个 ApplyMsg。这个过程发生在一个日志条目在所有服务器上达成一致之后，但在应用到状态机之前。
