# GFS

[TOC]

## Abstract

我们设计并实现了 Google 文件系统，这是一种适用于大型分布式数据密集型应用程序的可扩展(scalable)的分布式文件系统。它在廉价的商业硬件上运行时提供容错能力fault tolerance，并为大量客户端提供高聚合性能high aggregate performance 。

虽然与以前的分布式文件系统有许多相同的目标，但我们的设计的初衷源于我们观察到当前和预期的应用程序工作负载和技术环境与早期文件系统的假设出现了明显的背离。 这促使我们重新审视传统选择并探索截然不同的设计点。

该文件系统已经成功满足了我们的存储需求。 它在 Google 内部广泛部署，作为我们服务所用数据的生成和处理以及需要大数据集的研发工作的存储平台。 迄今为止最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储，并且它被数百个客户端同时访问。

在本文中，我们介绍了旨在支持分布式应用程序的文件系统接口扩展 file system interface extensions，讨论了我们设计的许多方面，并报告了micro-benchmarks和实际使用的测量结果。

*Benchmarking is an essential tool in development across languages, but choosing the correct scale can prove difficult.*

## INTRODUCTION

因此GFS与以往的分布式文件系统一样，追求高性能、可扩展性、可靠性和可用性，但它的设计与传统文件系统设计有很大不同。

文中列举了GFS设计时需要考虑的几个关键点：

1. 组件故障是常态而不是例外，需要通过监控、错误检测、容错和自动恢复来维护系统的稳定性。
2. 大多数文件都非常大，且通常包含许多应用对象，因此需要重新审视 I/O 操作和块大小等设计参数。
3. 大多数文件都是通过追加新数据而非覆盖现有数据来修改的，因此 GFS 的设计需要专注于追加操作的性能优化和原子性保证。
4. 应用程序和文件系统 API 的共同设计可以增加系统的灵活性。

## Design Overview

### 2.1 假设

GFS的设计受到了多个假设的指导，其中包括：

- 系统由大量的廉价组件构建，故障是常态而非例外。因此，系统必须具备不断监控、错误检测、容错和自动恢复的能力。
- 系统存储少量大型文件。文件数量在几百万个左右，每个文件通常大小为100 MB或更大。大型文件是常见的情况，小型文件需要支持，但无需优化。
- 工作负载主要包括两种读取方式：大规模流式读取和小规模随机读取。大型流式读取通常会读取数百KB、甚至1 MB或更多的数据，而来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移处读取几KB的数据。
- 工作负载还包括许多大型顺序写入，这些写入将数据附加到文件中。典型的操作大小与读取相似。一旦写入，文件很少再次修改。虽然支持在文件的任意位置进行小型写入，但无需优化。
- 系统必须有效地实现为多个客户端定义的语义，这些客户端同时附加到同一文件。我们的文件经常用作生产者-消费者队列或多路合并。数百个生产者，每个机器一个，将同时附加到一个文件中。具有最小同步开销的原子性是必要的。文件可能稍后被读取，或者消费者可能同时通过文件进行读取。
- 高持续带宽比低延迟更为重要。我们的大多数目标应用程序都非常看重高速率的批量处理数据，而很少对单个读取或写入的响应时间有严格的要求。

### 2.2 Interface:

- GFS provides a familiar file system interface, organized hierarchically in directories and identified by path-names.
- GFS supports the usual operations to create, delete, open, close, read, and write files.
- GFS has snapshot and record append operations that are useful for implementing multi-way merge results and producer-consumer queues.
- Snapshot creates a copy of a file or a directory tree at low cost.
- Record append allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity of each individual client's append.

GFS提供了一个熟悉的文件系统接口，尽管它没有实现类似于POSIX的标准API。文件按照目录层次结构组织并通过路径名识别。

我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。

此外，GFS还具有快照和记录追加操作

1. 快照以低成本创建文件或目录树的副本
2. 记录追加允许多个客户端并发地向同一文件追加数据，同时保证每个单独客户端的追加的原子性。它对于实现多路合并结果和多个客户端可以同时追加而无需额外锁定的生产者-消费者队列非常有用

### 2.3 Architecture:

- A GFS cluster consists of a single master and multiple chunkservers and is ==accessed by multiple clients.==

- **Each chunk is identified by an immutable and globally unique 64 bit chunk handle assigned by the master**

  这句话的意思是每个chunk都有一个64位的标识符（chunk handle），该标识符由GFS的master节点分配，用于唯一标识该chunk，并且该标识符在全局范围内是唯一的。"immutable"这个词意味着chunk handle是不可更改的，一旦分配，它就保持不变。由于chunk handle是唯一的，它可以用来在GFS集群中定位和操作特定的chunk

  

  

- The master maintains all file system metadata, including the namespace, access control information, and the mapping from files to chunks.

  

- GFS client code linked into each application implements the file system API and communicates with the master and chunkservers to read or write data on behalf of the application.

  这段话是说GFS客户端代码被链接到应用程序中实现文件系统API，并代表应用程序与主服务器和块服务器进行数据读写通信。客户端通过与主服务器交互来执行元数据操作，但所有与数据相关的通信都直接与块服务器进行。这意味着GFS不提供POSIX API，并且不需要与Linux vnode层进行连接。具体来说，客户端代码包含了GFS API的实现，应用程序使用这些API进行读写操作，客户端代码将这些操作转换为与GFS集群进行通信的消息。客户端通过向主服务器发送请求，获得有关文件系统元数据的信息，例如文件路径和块的位置，然后直接与块服务器进行数据通信，从而避免了在本地缓存数据时的一致性问题

  

  GFS客户端实现的文件系统API代码是由Google开发的，并与GFS主系统一起构成了整个GFS系统的一部分。这个API提供了访问和操作GFS文件的接口，客户端应用程序可以使用它来读写文件、创建文件、删除文件等。

  * 客户端与GFS主服务器进行交互以进行元数据操作，例如访问控制、文件与chunk的映射等，
  * 而所有的数据传输都直接在客户端和chunkservers之间进行

- **Neither the client nor the chunkserver caches file data**



### 2.5  single master

- GFS采用单一的Master节点，这简化了设计，并且允许Master使用全局知识做出复杂的chunk放置和复制决策。

- 为了避免Master成为瓶颈，我们必须将其在读写方面的参与最小化。客户端不会通过Master读写文件数据，而是询问Master应该联系哪些Chunkservers。客户端在有限的时间内缓存这些信息，并直接与Chunkservers进行大多数后续操作的交互。

  **并不缓存fille data，但是会缓存(key:filename,chunk index,value: chunk handle, chunk locations)**

- 对于简单的读取操作，客户端将应用程序指定的文件名和字节偏移量转换为文件中的chunk索引，然后向Master发送一个包含文件名和chunk索引的请求。Master会回复相应的chunk handle和副本的位置，客户端将这些信息使用文件名和chunk索引作为键进行缓存。

- 然后客户端向其中一个副本发送请求，请求指定了chunk handle和在该chunk内的字节范围。对于同一chunk的进一步读取操作不需要更多的客户端-主服务器交互，直到缓存的信息过期或重新打开文件。实际上，客户端通常在同一个请求中请求多个chunk，而Master也可以包括请求的chunk之后的chunk的信息(==依然是局部性原理==)。这些额外的信息几乎不需要额外的成本，可以避免几个未来的客户端-主服务器交互。

<img src="http://cdn.zhengyanchen.cn/img202303121450989.png" alt="截屏2023-03-12 14.50.36" style="zoom:60%;" />



## 2.5 chunk size

在设计GFS时，chunk size（数据块大小）是其中一个关键参数。GFS选择的chunk size为64 MB，比典型的文件系统块大小要大得多。每个chunk replica（数据块副本）都作为一个普通的Linux文件存储在chunkserver上，只有在需要时才会被扩展。延迟空间分配避免了因内部碎片而浪费空间(这也是对这么大的chunk size最大的反对意见)

### 这里的Lazy space allocation是具体指什么？

Lazy space allocation是指仅在需要时才分配空间的策略，也称为延迟分配空间。在GFS中，当一个chunk被创建时，它并不会占用整个chunk的大小，而是只占用必要的空间。在写入数据时，chunk会动态地增长以适应写入的数据，而在删除数据时则会自动收缩，释放空间。这种延迟分配空间的策略可以避免由于内部碎片而浪费空间。

### 我不太明白为什么会出现内部碎片

内部碎片是指分配的空间大小大于实际需要的空间大小时产生的浪费空间。在文件系统中，当文件大小不是块大小的整数倍时，就会产生内部碎片。举个例子，如果块大小为4KB，而一个文件大小为12KB，那么在磁盘上实际上会分配3个块（12KB/4KB=3），但是最后一个块只使用了不到4KB的空间，剩余的空间就被浪费了，这就是内部碎片。

在GFS中，采用了Lazy space allocation技术，只有在需要时才分配实际的存储空间，这样可以避免因分配过多空间而产生的内部碎片。例如，在写一个新的chunk时，只有在实际数据写入之前才会分配存储空间，这样就可以避免因为chunk大小过大而浪费过多的磁盘空间。

### 和内存的lazy  allocation很像

是的，Lazy space allocation 在某种程度上类似于内存中的 lazy allocation。在内存中，操作系统通常不会立即为程序分配所有请求的内存空间，而是等到程序需要时再动态分配。这种方法可以减少内存的浪费和内部碎片。

类似地，在文件系统中，如果使用的是固定大小的块，则在一个块中只存储一小部分数据时，该块中的未使用部分就会浪费，这就是内部碎片。为了避免这种浪费，Lazy space allocation 可以等到需要存储更多数据时才动态地分配磁盘空间，从而避免不必要的浪费。

### GFS一个chunk的大小远远大于一个4KB,这意味这Linux文件系统组织它的时候是会使用不连续的磁盘块对吗

是的，GFS中的一个chunk大小是64MB，远大于一个文件系统块大小（通常是4KB或者8KB）。因此，当GFS存储一个chunk时，它并不是连续地在磁盘上分配空间，而是使用不连续的磁盘块来组织这个chunk。这样做可以避免出现内部碎片，并且可以提高文件系统的性能，特别是对于大文件的读写。

较大的chunk size带来了几个重要的优点。

* 首先，它减少了客户端与主服务器的交互，因为在同一个chunk上的读写只需要向主服务器进行一次初始请求，获取数据块的位置信息即可。这种减少对于我们的工作负载尤其重要，因为应用程序主要是顺序读写大文件。即使是小的随机读取，客户端也可以轻松地缓存多TB工作集的所有块位置信息。
* 其次，由于在大的chunk上，客户端更有可能对同一chunk执行多次操作，因此可以通过在较长的时间内保持与chunkserver的持久TCP连接来减少网络开销
* 它减少了存储在主服务器上的元数据的大小。这使我们能够将元数据存储在内存中，进而带来我们将在2.6.1节中讨论的其他优点。

然而，即使使用延迟空间分配，较大的chunk size也存在缺点。小文件由少量的chunk组成，可能只有一个chunk。如果许多客户端正在访问同一个文件，则存储这些chunk的chunkservers可能会成为热点。在实践中，热点并不是一个主要问题，因为我们的应用程序主要顺序读取大型多块文件。 because our applications==mostly read large multi-chunk files sequentially.==

然而，当GFS首次被批处理队列系统使用时，热点问题确实出现了：一个可执行文件被写入GFS作为单个chunk文件，然后在数百台机器上同时启动。存储该可执行文件的少数chunkservers由数百个同时请求的客户端负载过重。我们通过使用更高的副本因子存储这些可执行文件并使批处理队列系统错开应用程序启动时间来解决了这个问题。潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。



## 2.6 元数据

GFS的master存储了三种重要的元数据：文件和chunk的命名空间，文件到chunk的映射，以及每个chunk副本的位置。

**在GFS中，命名空间用于存储文件和chunk的名称，以及它们在文件系统中的层次结构关系**

所有的元数据都保存在master的内存中。

前两种类型的元数据（命名空间和文件到chunk映射）也通过将操作记录到存储在master本地磁盘上的操作日志并在远程机器上进行复制来保持持久化。使用日志可以使我们简单、可靠地更新master状态，而不会在master崩溃时出现不一致的情况。

master不会持久地存储chunk位置信息。相反，它在master启动时和每当chunkserver加入集群时询问每个chunkserver的chunks信息。

### 2.6.1 内存数据结构

### master定期在后台扫描其整个状态，是指和所有的chunksever通讯嘛?

不是，这里指的是Master定期扫描自己存储的所有元数据，以便及时发现和处理存储集群中的问题，例如对于磁盘空间的使用和负载的均衡等。这样可以在后台完成这些维护任务，而不会影响到客户端请求的处理速度。和chunkserver的通讯是在Master启动和每次chunkserver加入集群时进行的，用于更新chunk的位置信息。

### 可是如果是自己扫描，而不和chunksever通讯，那怎么确定chunkserver fail了

实际上，Master定期扫描它在内存中维护的chunk和副本位置信息的列表，以便确定哪些chunkserver上的chunk不再需要，哪些chunkserver上的chunk需要重新复制以确保数据冗余性，哪些chunkserver上的chunk负载过重需要迁移。这个扫描的过程并不需要与所有chunkserver通信，而是通过本地内存中存储的数据来判断。当Master检测到一个chunkserver没有及时响应时，就会认为该chunkserver失效并采取相应的措施。

### 如何判断chunkserver上的chunk负载过重需要迁移

GFS使用两种指标来判断chunkserver上的chunk负载是否过重：

1. 磁盘空间利用率：当chunkserver上的磁盘空间使用率超过某个阈值时，GFS就认为该chunkserver的负载较重。这个阈值可以在系统中进行配置，通常默认为85%。
2. chunkserver的负载均衡：GFS会定期计算每个chunkserver的负载，并根据系统配置的负载均衡策略来调整chunk分配。例如，如果一个chunkserver上的负载过重，GFS可以将一些chunk迁移至其他chunkserver上以实现负载均衡。

当一个chunkserver的负载达到某个阈值时，GFS会触发chunk迁移操作，将部分chunk迁移到其他chunkserver上，以避免负载过重导致系统性能下降。

### 那么元数据里是否会维护chunkserver状态的数据结构?

GFS的master节点并没有维护每个chunkserver的状态数据结构，因此它并不知道chunkserver的具体状态信息。相反，每个chunkserver会定期向master汇报自己的chunk信息，包括自己存储了哪些chunk以及每个chunk的存储情况。master会利用这些汇报信息来了解整个集群的状态，并作出相应的负载均衡和容错措施。同时，master还会根据每个chunkserver的汇报信息更新文件和chunk的命名空间。



### 那么Master如何检测到chunkserver没有及时响应

当Master会向chunkserver定期发送心跳信号以检测其是否仍然可用。如果一个chunkserver在一定时间内没有响应心跳，则Master会将其标记为不可用，并将其上的所有chunk标记为不可用。此外，如果一个chunkserver在处理请求时出现错误，它也会将其标记为不可用。被标记为不可用的chunkserver上的chunk将被重新复制到其他可用的chunkserver上以保证数据的可靠性。

这种仅存储在内存中的方法的一个潜在问题是，chunk数量和整个系统的容量受制于master的内存大小。但实际上这不是一个严重的限制。master为每个64 MB chunk保留少于64个字节的元数据。大多数chunk都是满的，因为大多数文件包含许多chunk，只有最后一个chunk可能部分填充。同样地，文件命名空间数据通常每个文件仅需要少于64字节，因为它使用前缀压缩紧凑地存储文件名。

如果需要支持更大的文件系统，则增加master的额外内存的成本是我们付出的一个很小的代价，因为将元数据存储在内存中带来的简单性、可靠性、性能和灵活性是我们所获得的。



### 2.6.2 Chunk Locations

**Master节点不会持久记录哪些chunkserver拥有某个chunk的副本。它只是在启动时轮询chunkservers以获取这些信息。**



* 之后，Master节点可以保持自我更新，因为它控制所有的chunk布局，并使用定期的**心跳消息**（HeartBeat）监视chunkserver的状态。

我们最初尝试在Master节点上持久地保留chunk位置信息，但我们决定在启动时从chunkservers请求数据，并定期更新，这样更简单。

* 这消除了在chunkservers加入和离开集群、更改名称、故障、重新启动等情况下保持Master节点和chunkservers同步的问题。在一个拥有数百台服务器的集群中，这些事件经常发生。

* 另一种理解这个设计决策的方式是意识到一个chunkserver有最终的决定权，决定它在自己的磁盘上是否有某个chunk。试图在Master节点上维护这些信息的一致性是没有意义的，因为chunkserver上的错误可能导致chunk自发消失（例如，磁盘可能损坏并被禁用），或者操作员可能会重命名chunkserver。

### 2.6.3 Operation log

- 操作日志包含关键元数据更改的历史记录，是GFS的核心。
- 操作日志不仅是元数据的唯一持久记录，而且还作为定义并发操作顺序的逻辑时间线。

#### 操作日志只会保存元数据里面的chunk和文件的信息对吗

是的，操作日志只保存元数据的变更信息，即文件和chunk的创建、删除、移动、复制、版本更新等操作。因为操作日志是用来恢复元数据状态的，所以只需要保存元数据的变更历史即可。而chunk的读写操作不会被记录在操作日志中，因为这些操作并不会改变元数据的状态。

#### 每次对chunk的修改都会导致版本号的更新是嘛

是的，每次对chunk的修改都会导致版本号的更新。GFS采用了乐观并发控制（optimistic concurrency control）的方式来处理多个client对同一个chunk的并发修改请求。这种方式下，每个client在请求修改某个chunk时，都会将其持有的最新版本号一并提交给master进行校验。==如果该版本号比master记录的最新版本号还要新，说明该client持有的版本已经过时，需要先更新到最新版本== **这里的描述似乎有点问题**。一旦master确认请求合法，就会为该chunk分配一个新的版本号，并将该版本号一并返回给client。client在后续对该chunk进行修改时，必须使用最新的版本号，否则修改请求将被拒绝。这种方式下，每个chunk都有一个唯一的版本号，可以追踪其变化历史。

#### 什么叫optimistic concurrency control

Optimistic concurrency control (OCC) 是一种多用户并发控制技术，用于确保同时对同一数据项进行的操作不会导致数据的冲突和损坏。相比于悲观并发控制，OCC 不会对所有可能的冲突情况都进行加锁或者阻塞等等严格限制，而是采取乐观的策略。即假设并发操作不会互相干扰，只在提交时检查是否发生冲突，如果发现了冲突，则回滚事务。

在数据存储系统中，OCC 通常用于实现乐观并发控制，以避免对系统性能的不利影响。OCC 的核心思想是在每个操作中维护一个版本号，并且只有在提交操作时才会检查版本号，从而减少锁竞争和系统调度的开销。如果两个并发的操作修改了同一个数据项，那么提交操作时就会发生冲突，其中一个操作会被回滚并返回错误，由于大多数操作都不会冲突，因此 OCC 可以实现较高的并发性能和吞吐量。

#### 也就是说说client去读某chunk的时候不需要经过master,但是写chunk的时候就要将client的版本号交给master校验？

是的，当客户端要写入一个chunk时，它首先向master请求chunk的位置信息和最新版本号。然后客户端使用这些信息来向chunkserver写入数据，并将最新版本号作为写操作的一部分发送回master。Master会比较客户端提供的版本号与当前记录在操作日志中的版本号，如果匹配则表示客户端的操作是有效的，可以执行，Master会将该操作写入操作日志并将其复制到其他的机器上。如果版本号不匹配，则Master会拒绝客户端的写入请求，通知客户端重试。这种机制被称为乐观并发控制（Optimistic Concurrency Control）。

#### 所以过程是这样的：client对一个文件的某个位置发出写请求，master会确定该文件位置对应的chunk, 然后给该chunk分配一个新的版本号，然后将该chunk的位置信息和更新后的版本号发回给client，client修改该chunk以后，client会发出版本号给master，如果匹配，则master会将该写操作写入操作日志，你看对吗

是的，你说的过程大致正确。客户端首先向主服务器发出写请求，主服务器确定该请求对应的chunk，并为该chunk分配一个新的版本号。主服务器会将chunk的位置信息和新的版本号发送给客户端，以便客户端更新该chunk。客户端写入chunk后，会向主服务器发送其写操作的版本号。如果该版本号与主服务器上的版本号匹配，主服务器会将该写操作写入操作日志。



- 为了确保操作日志的可靠性，必须在将元数据更改持久化之前，将其可靠地存储，并且对客户端操作进行响应，只有在将相应的日志记录在本地和远程磁盘上刷新后才能使更改对客户端可见。

- 操作日志被复制到多台远程机器上以确保其可靠性。

  

- Master通过重放操作日志来==恢复其文件系统状态==

  日志需要尽可能小



#### 什么叫The master recovers its file system state by replaying the operation log.

这句话的意思是，当GFS的Master节点需要恢复文件系统的状态时，它会通过按顺序重新执行操作日志中的记录来重建整个文件系统状态。也就是说，Master会遍历操作日志中的每一条记录，按照记录的先后顺序对文件系统进行恢复，以达到与故障前相同的状态。

#### Master节点需要恢复文件系统的状态时，它会通过按顺序重新执行操作日志中的记录来重建整个文件系统状态，所以log会用链表来组织

是的，当Master节点需要恢复文件系统的状态时，它会按照操作日志中的记录顺序来重建整个文件系统状态。因此，操作日志需要被组织成一个可顺序访问的数据结构，链表是其中一种常见的实现方式。在执行操作日志时，Master节点将逐一执行每个记录，并将文件系统状态还原到该记录执行后的状态。



- Master在日志增长到一定大小时检查点其状态以便快速恢复，Master以批处理的方式将多个日志记录一起刷新到磁盘，减少对系统吞吐量的影响。
- Master切换到一个新的日志文件并在单独的线程中创建新的检查点，该检查点包括切换之前的所有突变。

#### 当Master切换到一个新的日志文件，它会删除之前内存的log对吧



是的，当Master切换到一个新的日志文件时，它通常会删除之前内存中的日志，因为之前的日志已经被写入到磁盘的检查点文件中。这样可以释放内存，防止日志文件过大导致内存不足。但是，如果Master需要重新加载之前的日志来进行数据恢复，它可以将检查点文件加载到内存中，然后再重放日志文件，从而恢复文件系统状态。



#### 检查点的意思

在分布式系统中，检查点是指保存系统状态的时刻或快照。它可以用于系统恢复和备份，以减少恢复时间和成本。在GFS中，Master会定期将操作日志转换为一组检查点，以保持日志的规模较小，加快系统的恢复速度。当Master重启时，它可以从最新的检查点开始恢复，而不是从头开始回放整个操作日志，这可以显著减少恢复时间。检查点通常也会复制到其他节点上，以提高可靠性。

#### 所以操作日志是在放在内存里，而检查点在磁盘

是的，操作日志是在内存中缓存的，而检查点是保存在磁盘上的。操作日志用于记录元数据的变化，是一个追加写入的日志文件。而检查点则是用来保存元数据的一个快照，通常是以一种可以直接映射到内存中进行快速查询的形式保存在磁盘上。由于操作日志文件通常比较大，所以检查点可以帮助加快系统的恢复速度。Master会定期地在内存中生成一个新的检查点，并将其写入磁盘中，同时清空操作日志文件，以减少其对系统性能的影响。



#### 检查点是如何按照B树组织的

在GFS中，文件系统的元数据使用B树进行组织，检查点也是按照B树的结构进行组织的。具体来说，每个B树节点都有一个对应的磁盘块，该磁盘块存储了节点的所有关键字和指向子节点的指针。对于每个B树节点，其子节点的指针和关键字数目必须小于等于一个固定的上限值，这个上限值通常称为节点的阶。因此，节点大小也是一个固定的上限值。

在GFS中，为了方便管理和维护B树的节点，Master会将所有的B树节点按照节点在树中的深度和节点ID进行排序，并将它们组织成一棵B树。Master在内存中维护了一份B树的副本，并将检查点信息定期地写入磁盘，以便在系统崩溃时进行恢复。当Master需要创建一个新的检查点时，它会从内存中的B树开始遍历，并将每个B树节点写入到磁盘中。由于B树节点的大小是固定的，因此每个节点都可以完全写入到一个磁盘块中，从而提高了写入效率。



#### 以下是一个简单的 Python 代码示例，演示了如何使用 B 树组织检查点文件：

```python
import pickle
import os

# 定义 B 树节点类
class BTreeNode:
    def __init__(self, is_leaf=False):
        self.keys = []
        self.values = []
        self.is_leaf = is_leaf
        self.children = []

    # 保存节点到文件中
    def save(self, fp):
        pickle.dump(self.keys, fp)
        pickle.dump(self.values, fp)
        pickle.dump(self.is_leaf, fp)
        pickle.dump(len(self.children), fp)
        for child in self.children:
            child.save(fp)

    # 从文件中加载节点
    @staticmethod
    def load(fp):
        node = BTreeNode()
        node.keys = pickle.load(fp)
        node.values = pickle.load(fp)
        node.is_leaf = pickle.load(fp)
        child_count = pickle.load(fp)
        for i in range(child_count):
            node.children.append(BTreeNode.load(fp))
        return node

# 定义 B 树类
class BTree:
    def __init__(self, checkpoint_file_path):
        self.checkpoint_file_path = checkpoint_file_path
        self.root = None

        # 如果检查点文件存在，则从文件中加载 B 树
        if os.path.exists(self.checkpoint_file_path):
            with open(self.checkpoint_file_path, 'rb') as f:
                self.root = BTreeNode.load(f)

    # 将 B 树保存到检查点文件中
    def checkpoint(self):
        with open(self.checkpoint_file_path, 'wb') as f:
            self.root.save(f)

# 使用示例
btree = BTree('checkpoint.dat')
if btree.root is None:
    btree.root = BTreeNode(is_leaf=True)
    btree.root.keys = [1, 2, 3]
    btree.root.values = ['a', 'b', 'c']
    child1 = BTreeNode(is_leaf=True)
    child1.keys = [4, 5]
    child1.values = ['d', 'e']
    child2 = BTreeNode(is_leaf=True)
    child2.keys = [6, 7]
    child2.values = ['f', 'g']
    btree.root.children = [child1, child2]
btree.checkpoint()

```

上示例定义了一个 `BTreeNode` 类和一个 `BTree` 类。`BTreeNode` 类表示 B 树中的一个节点，包含该节点的键、值、是否是叶子节点以及其子节点。`BTree` 类表示 B 树，包含一个检查点文件路径和根节点。

在示例中，我们先判断检查点文件是否存在，如果存在，则从检查点文件中加载 B 树。如果检查点文件不存在，则创建一个空的根节点，并添加两个子节点，然后将 B 树保存到检查点文件中。

值得注意的是，在保存节点到文件中和从文件中加载节点时，我们使用了 Python 的 `pickle` 序列化模块，以便将节点对象转换为字节序列，并将其写入到文件中。在实际应用中，我们还需要考虑文件读写的并发访问问题，以及如何保证检查点的一致性和可靠性等问题。

- 旧的检查点和日志文件可以自由删除，尽管我们会保留几个以防灾难发生。

## 2.7 Consistency Model



GFS采用宽松的一致性模型

###  2.7.1 GFS的保证

文件名称空间的变化（例如文件创建）是原子的。它们由master独占性地处理：

名称空间锁定保证了原子性和正确性（第4.1节）；

主节点的操作日志定义了这些操作的全局总顺序（第2.6.3节）。 

数据变异后的文件区域的状态取决于变异类型、成功或失败以及是否存在并发变异。表1总结了结果。

<img src="http://cdn.zhengyanchen.cn/img202303131425050.png" alt="截屏2023-03-13 14.25.29" style="zoom:70%;" />



一个文件数据变异（例如写入）后，文件区域的状态会依赖于变异的类型、成功或失败以及是否有并发变异的存在。表1总结了不同情况下的结果。在这个上下文中，“一致性”指的是所有客户端看到的数据始终相同，而“定义”（==defined==）则指区域是一致的，并且客户端将完全看到变异写入的内容。如果变异成功且没有并发写入的干扰，受影响的区域就是被定义的（且暗含着一致性）：所有客户端始终会看到变异所写的内容。

并发成功的变异会使该区域变得“未定义”但一致：

#### 什么叫Concurrent successful mutations leave the region undefined but consistent:？

这句话的意思是，如果有多个写操作同时成功地修改了同一个文件区域，那么该区域将会是“未定义的”状态，但是它仍然是一致的。也就是说，所有客户端看到的数据都是相同的，但可能是由多个不同的写操作所产生的混合结果，因此无法确定哪个写操作导致了该区域的数据。在这种情况下，该区域被称为“未定义的”，但是它是一致的，因为所有客户端看到的数据都是相同的。

失败的变异会使该区域不一致（因此也未定义）：不同的客户端可能在不同的时间看到不同的数据。

接下来，我们将描述如何让应用程序区分定义的区域和未定义的区域，而应用程序不需要进一步区分不同类型的未定义区域。

数据变异可以是**写操作**或**记录追加**操作。写操作会在应用程序指定的文件偏移处写入数据。记录追加操作会在至少一次原子追加数据（“记录”）

#### 写操作或记录追加操作的差别是什么？

写操作和记录追加操作是GFS中两种不同的数据变异操作。

写操作会在应用程序指定的文件偏移量处写入数据。这意味着如果一个客户端发起了多次写操作，那么这些操作将覆盖同一文件偏移量上的数据。而记录追加操作则是在文件末尾原子性地追加数据（即“记录”），即使在并发数据变异的情况下也能保证至少追加一次。GFS会为记录追加操作选择一个偏移量，并将该偏移量返回给客户端，以表示该记录的开头。

#### 这里的 atomically at least once 是什么意思

这里的 "atomically at least once" 指的是记录追加（record append）操作，其保证在存在并发变异的情况下，至少会将记录追加一次（即使是在故障恢复的情况下），从而保证数据的一致性和完整性。这意味着，如果有多个客户端并发地追加记录到同一个文件，每个客户端都能够在其追加的记录后看到其他客户端追加的记录，而不会丢失任何数据。



此外，GFS可能会在其中插入填充或记录副本，它们占据被认为是不一致的区域，通常被用户数据所掩盖。

#### 通俗地讲讲这句 ，GFS可能会在其中插入填充或记录副本。它们占据被认为是不一致的区域，通常被用户数据所掩盖。

这句话的意思是，当GFS执行记录追加操作时，它可能会在记录之间插入一些额外的数据，这些数据被称为“填充”或“记录副本”。这些额外的数据会占据一些不一致的区域，但是这些区域通常只占用很小的空间，被用户数据所掩盖，因此对用户不会产生影响。这样做的目的是为了在发生并发写入时保证记录的原子性，从而确保记录的完整性。

#### 为什么要填充

在 GFS 中，数据被分成大小固定的块（chunk），每个块都被复制到多个 chunkserver 上以提高可靠性。如果一条记录的大小小于一个块的大小，而写入的偏移量不在块的结尾处，那么块中剩余的空间将浪费掉。为了充分利用每个块的空间，GFS 可能会在记录末尾插入填充，从而使得块中的空间被充分利用。

此外，填充还可以帮助提高性能。GFS 在处理大文件时会采用预读取机制，即在客户端请求的文件数据周围读取更多的数据，以减少磁盘寻道的次数。如果一条记录的大小小于块大小，那么填充可以在记录末尾提供一些额外的数据，这些数据可能会被预读取并缓存在客户端中，从而减少磁盘 I/O 操作的次数，提高读取性能。

#### 这里的填充是保证append的结尾能块对齐还是append的开头能块对齐

在 GFS 中，填充是用来保证 append 操作的结尾对齐的。当一个 client 发送一个 append 请求时，GFS 会将数据以块为单位写入磁盘。如果最后一个块不能完全装下这次 append 的数据，那么 GFS 就会在块的末尾插入一些填充数据，使得最后一个块达到块大小并对齐到块的边界。这样做可以避免最后一个块出现不完整的情况，也能够保证下一次 append 请求的开始位置对齐到块的边界。

#### 填充是如何保证在发生并发写入时记录的原子性的

填充并不直接保证并发写入的原子性。填充的主要作用是为了使得每个记录的偏移量（offset）是固定的，从而方便客户端读取数据，而不必担心偏移量的变化。对于记录追加（record append）操作，填充可以使得每个记录（record）在对应的块（chunk）中占据一个固定的区域，因此读取时可以直接按块读取，而不必关心块内的记录偏移量。对于普通的写操作，填充也可以使得每次写入的数据都能够对齐到块的边界上，方便后续读取。

具体来说，当发生并发写入时，GFS会使用一些其他机制来保证原子性，例如使用chunk版本号来检测副本之间的不一致性并且在发现不一致时禁止访问该副本。在所有副本上按照同样的顺序执行操作，从而保证所有副本上的数据是一致的。这些机制可以确保并发写入的原子性，而填充则是为了方便读取而引入的概念。



在一系列成功的变异之后，变异的文件区域保证已定义并包含最后一个变异写入的数据。 GFS通过

1. 在所有副本上以相同的顺序将变异应用于块（第3.1节），
2. 使用块版本号来检测已变为过时的任何副本，因为其块服务器在其关闭时错过了变异（第4.5节）来实现这一点。

*过时的副本将永远不会参与变异或被提供给向主服务器请求块位置的客户端。它们将在最早的机会被垃圾收集。*

由于客户端缓存了块位置，因此在刷新该信息之前，它们可能会从已过时的副本中读取。该窗口由缓存条目的超时和文件的下一次打开限制，后者会从缓存中清除该文件的所有块信息。此外，由于我们的大多数文件仅追加，因此已过时的副本通常返回过早结束的块而不是过时的数据。当读取器重试并联系主服务器时，它将立即获得当前块位置。

在成功变异之后很久，组件故障当然仍可能会损坏或破坏数据。 GFS通过主服务器和所有块服务器之间的定期握手来识别失败的块服务器，并通过校验和（第5.2节）检测数据损坏。一旦问题浮出水面，数据将尽快从有效副本中恢复（第4.3节）。仅当GFS无法及时做出反应（通常在几分钟内）并且所有副本都丢失时，块才会被永久性地丢失。即使在这种情况下，它也会变得不可用，而不是受损：应用程序会收到明确的错误消息而不是受损的数据。







